XG Boost for Regression -eXtreme Gradienet Boost

desinged for large and complicated dataset.

In boosting, the base learners are used in sequentually.

The basic principle behind the working of the boosting algorithm is to generate multiple weak learners and combine their predictions to form one strong rule. 
These weak rules are generated by applying basic Machine Learning algorithms on different distributions of the data set. 
These algorithms generate weak rules for each iteration. 
After multiple iterations, the weak learners are combined to form a strong learner that will predict a more accurate outcome.

Step 1: The base algorithm reads the data and assigns equal weight to each sample observation.

Step 2: False predictions made by the base learner are identified. In the next iteration, these false predictions are assigned to the next base learner with a higher weightage on these incorrect predictions.

Step 3: Repeat step 2 until the algorithm can correctly classify the output.

Therefore, the main aim of Boosting is to focus more on miss-classified predictions.

Types of boosting algorithms
1.Adaptive Boosting or AdaBoost
2.Gradient Boosting
3.XGBoost (Extreme Gradient Boosting)

Gradient Boosting is based on sequential ensemble learning(boosting). 
Here the base learners are generated sequentially in such a way that the present base learner is always more effective than the previous one, 
i.e. the overall model improves sequentially with each iteration.

XGBoost is an advanced version of Gradient boosting method, it literally means eXtreme Gradient Boosting. 
XGBoost developed by Tianqi Chen, falls under the category of Distributed Machine Learning Community (DMLC).

to install : pip install xgboost

The main aim of this algorithm is to increase the speed and efficiency of computation. 
The Gradient Descent Boosting algorithm computes the output at a slower rate since they sequentially analyze the data set, 
therefore XGBoost is used to boost or extremely boost the performance of the model.