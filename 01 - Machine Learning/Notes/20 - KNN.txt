KNN - K Nearest Neighbour

This is more of a statstical algorithm.
k- number of neighbours to be considered.
we have the same properties of our neighbours.

the majority class of neighbourhood will be assigned to the unknown observation.

always keep k as an odd number. Even number can cause a tie.

This can be used for classification aspects, but it is a not parametric lazy modelling.

This can also used for missing value handling. (###Important Aspect)
Assume age is missing, age can impute using other columns by implementing KNN algorithm. 
In this case the aggregate(mean,median) of age of neighbours is considered.


Advantages:
No assumptions about data — useful, for example, for nonlinear data
Simple algorithm — to explain and understand/interpret
Versatile — useful for classification or regression
can used missing value handling (missing value imputation)

KNN Imputer
Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. 
Two samples are close if the features that neither is missing are close. 
By default, an euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors.