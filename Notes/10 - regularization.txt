Bias Variance Trade off

There are 2 error can be calculated.
1.Testing Error/Evaluation error
	Ytest vs Ypred

2.Training error - we can predict the result for Xtrain which is already used for training.
	model.fit(X_train,y_train)
	ytrainpred=model.predict(X_train)
	MSE(ytrainpred,y_train)

So in general we have to Error calculation such as Training Error and Testing Error.

we consider/accept a model when the training error is minimum(max accuracy)

ideal situation - best model
	Low Training Error and Low Evaluation Error

Other situations
	Low Training Error and High Evalutaion Error 
		- This situation is known as Overfitting.
		- The model is well trained. but not explained every possibilities
		- The model is not generic

	High Training Error and High Evalution Error
		- underfitting
		- Change the algorithm (this is the best option)
		- Use more data field/data
	High training error and low evalution error - void situation

Overfitting happens when a model learns the detail and noise in the training data 
to the extent that it negatively impacts the performance of the model on new data. 
This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. 
The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.


BIAS - Bias is the difference between the average prediction of our model 
	and the correct value which we are trying to predict while training. (nothing but the Error)

VARIANCE -  Variance is the variability of model prediction for a given data point or a value 
	which tells us spread of our data.

a best model will have Low Bias(low training error) and Low Variance(low testing error)


if bias is low - low Training Error
if bias is high - high training error
if variance is low - EE is same as TE
if variance is high - EE is not same as TE

					Bias
				Low		High

		Low	low TE,low EE		high TE,high EE		

Variance

		High	low TE,High EE		Not Exisiting(high TE,low EE)


Right Model - Low Bias,Low Variance
Underfit - High Bias,Low Variance
Overfit - Low Bias, High Variance

How to handle underfitting
	Use better algorithm
	Use more data

How to handle Overfitting
	(we have enough data and the model is also good)
	There are many techniques to avoid/minimize overfitting
	1.Regularization - mostly used for linear models
		a.Ridge Regression
		b.Lasso Regression
		c.Elastic Net
	2.Ensemble Models - used for nonliner models
		a.Bagging - Random Forest algorithm
		b.Boosting - XG Boost algorithm,Ada boost algorithm

Reasons for Overfitting

Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.

Data level techniqiues for prevnet overfitting
Cross Validation  - (k-fold cross-validation)
	- assume that the data will be splitted into k chunks, say 10 chunks.
	The model will be trained in 10 iterations using 9 chunks for training and remaining 1 for testing.
	S1- training 1-9 evaluation 10
	S2 - training 2-10 evaluation 1
	S3 - training 3-10,1  evaluation 2
	S10 - training 1-8,10 evaluation 9

What is Regularization - Regularization is a technique used for tuning the function by adding an additional penalty term in the error function.
	tuning  of function means minimizing the error and optimize m,c.


